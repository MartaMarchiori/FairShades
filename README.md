# FairShades: Fairness Auditing via Explainability in Abusive Language Detection Systems

FairShades is a model-agnostic approach for auditing the outcomes of abusive language detection systems. 
Combining explainability and fairness evaluation, it can identify unintended biases and sensitive categories towards which the models are most discriminative. 
This objective is pursued through the auditing of meaningful counterfactuals generated through the [CheckList](https://github.com/marcotcr/checklist) framework (Ribeiro et al., 2020).
We conduct several experiments on BERT-based models to demonstrate our proposal's novelty and effectiveness for unmasking biases.

This project is the continuation of a master's thesis project supervised and developed with Riccardo Guidotti, University of Pisa.
